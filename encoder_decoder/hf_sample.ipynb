{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitenvvenvde88aacb140049cab59668ffa038508f",
   "display_name": "Python 3.6.9 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: datasets==1.0.2 in /home/ryan/ml/env/lib/python3.6/site-packages (1.0.2)\nRequirement already satisfied: pyarrow>=0.17.1 in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (2.0.0)\nRequirement already satisfied: tqdm>=4.27 in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (4.48.2)\nRequirement already satisfied: requests>=2.19.0 in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (2.23.0)\nRequirement already satisfied: xxhash in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (2.0.0)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (0.7)\nRequirement already satisfied: numpy>=1.17 in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (1.18.1)\nRequirement already satisfied: pandas in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (1.0.1)\nRequirement already satisfied: dill in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (0.3.3)\nRequirement already satisfied: filelock in /home/ryan/ml/env/lib/python3.6/site-packages (from datasets==1.0.2) (3.0.12)\nRequirement already satisfied: idna<3,>=2.5 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.0.2) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.0.2) (1.25.8)\nRequirement already satisfied: certifi>=2017.4.17 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests>=2.19.0->datasets==1.0.2) (2019.11.28)\nRequirement already satisfied: pytz>=2017.2 in /home/ryan/ml/env/lib/python3.6/site-packages (from pandas->datasets==1.0.2) (2019.3)\nRequirement already satisfied: python-dateutil>=2.6.1 in /home/ryan/ml/env/lib/python3.6/site-packages (from pandas->datasets==1.0.2) (2.8.1)\nRequirement already satisfied: six>=1.5 in /home/ryan/ml/env/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->datasets==1.0.2) (1.13.0)\n\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\nYou should consider upgrading via the '/home/ryan/ml/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: transformers in /home/ryan/ml/env/lib/python3.6/site-packages (3.4.0)\nRequirement already satisfied: numpy in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: sacremoses in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: tqdm>=4.27 in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (4.48.2)\nRequirement already satisfied: sentencepiece!=0.1.92 in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (0.1.94)\nRequirement already satisfied: tokenizers==0.9.2 in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (0.9.2)\nRequirement already satisfied: regex!=2019.12.17 in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (2020.7.14)\nRequirement already satisfied: protobuf in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (3.11.2)\nRequirement already satisfied: requests in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied: packaging in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (20.4)\nRequirement already satisfied: filelock in /home/ryan/ml/env/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: click in /home/ryan/ml/env/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\nRequirement already satisfied: joblib in /home/ryan/ml/env/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: six in /home/ryan/ml/env/lib/python3.6/site-packages (from sacremoses->transformers) (1.13.0)\nRequirement already satisfied: setuptools in /home/ryan/ml/env/lib/python3.6/site-packages (from protobuf->transformers) (46.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\nRequirement already satisfied: idna<3,>=2.5 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /home/ryan/ml/env/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: pyparsing>=2.0.2 in /home/ryan/ml/env/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\nYou should consider upgrading via the '/home/ryan/ml/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "!pip install datasets==1.0.2\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info.description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from datasets import ClassLabel\n",
    "\n",
    "df = pd.DataFrame(train_data[:1])\n",
    "del df[\"id\"]\n",
    "for column, typ in train_data.features.items():\n",
    "      if isinstance(typ, ClassLabel):\n",
    "          df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map article and summary len to dict as well as if sample is longer than 512 tokens\n",
    "def map_to_length(x):\n",
    "  x[\"article_len\"] = len(tokenizer(x[\"article\"]).input_ids)\n",
    "  x[\"article_longer_512\"] = int(x[\"article_len\"] > tokenizer.max_len)\n",
    "  x[\"summary_len\"] = len(tokenizer(x[\"highlights\"]).input_ids)\n",
    "  x[\"summary_longer_64\"] = int(x[\"summary_len\"] > 64)\n",
    "  x[\"summary_longer_128\"] = int(x[\"summary_len\"] > 128)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "data_stats = train_data.select(range(sample_size)).map(map_to_length, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_print_stats(x):\n",
    "  if len(x[\"article_len\"]) == sample_size:\n",
    "    print(\n",
    "        \"Article Mean: {}, %-Articles > 512:{}, Summary Mean:{}, %-Summary > 64:{}, %-Summary > 128:{}\".format(\n",
    "            sum(x[\"article_len\"]) / sample_size,\n",
    "            sum(x[\"article_longer_512\"]) / sample_size, \n",
    "            sum(x[\"summary_len\"]) / sample_size,\n",
    "            sum(x[\"summary_longer_64\"]) / sample_size,\n",
    "            sum(x[\"summary_longer_128\"]) / sample_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "output = data_stats.map(\n",
    "  compute_and_print_stats, \n",
    "  batched=True,\n",
    "  batch_size=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch[\"input_ids\"] = inputs.input_ids\n",
    "  batch[\"attention_mask\"] = inputs.attention_mask\n",
    "  batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "  batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "  batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "\n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.select(range(32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4996336ad82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data.set_format(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decoder_input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decoder_attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.select(range(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EncoderDecoderModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.save_pretrained(\"bert2bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert = EncoderDecoderModel.from_pretrained(\"bert2bert\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm seq2seq_trainer.py\n",
    "!rm seq2seq_training_args.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_trainer.py\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/seq2seq/seq2seq_training_args.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git-python==1.0.3\n",
    "!pip install rouge_score\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_trainer import Seq2SeqTrainer\n",
    "from seq2seq_training_args import Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=10,\n",
    "    eval_steps=4,\n",
    "    # logging_steps=1000,\n",
    "    # save_steps=500,\n",
    "    # eval_steps=7500,\n",
    "    # warmup_steps=2000,\n",
    "    # save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = datasets.load_metric(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bert2bert = EncoderDecoderModel.from_pretrained(\"./checkpoint-20\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}